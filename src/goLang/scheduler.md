# Планировщик

Выделяют 3 модели для нарезания вычислений на потоки. 

- **N:1** - несколько пользовательских потоков запущено на едином потоке ядра операционной системы. Этот способ имеет то преимущество, что осуществляется очень быстрое переключение контекстов, но нет возможности воспользоваться преимуществами многоядерных систем. 

- **1:1** - каждый пользовательский поток выполнения совпадает с одним потоком операционной системы. Он использует все ядра автоматически, но переключение контекста происходит медленно, потому что требует прерываний работы операционной системы.

- Go пытается взять лучшее из обоих миров с помощью **М:N** планировщика. При этом произвольное число Go-рутин **M** планируется на произвольное количество потоков **N** операционной системы. Этим вы получаете одновременно быстрое переключение контекста, и возможность воспользоваться всеми ядрами в вашей системе. Основным недостатком данного подхода является сложность его включения в планировщик.  

Планировщик Go использует 3 сущности:

![img](../media/go/scheduler_mpg.jpeg)

**M Machine**(Треугольник) thread операционной системы. Точнее абстракция над ним, в go runtime представлена в виде структуры. Выполнением такого потока управляет операционная система, и работает это во многом подобно вашим стандартным потокам POSIX. 

- M создаются по мере необходимости (но ограничены `GOMAXPROCS`).
- Если горутина блокируется в syscall, создается новый M.
- System Monitor завершает неиспользуемые M через `forcegc` (чтобы не накапливались лишние потоки).



**G Goroutine**(Круг). Он включает стек, указатель команд и другую важную информацию для планирования горутины, такую как канал, который на ней может быть блокирован. 



**Прямоугольник(P Processor) - Планировщик на ядре**. Можно понимать как локальную версию планировщика c собственной очередью горутин под конкретный  **M** c каким-то контекстом(регистры и т.д). 



В планировщике Go есть две разные очереди выполнения:

- глобальная (**GRQ**) 

- локальная (**LRQ**) - может хранит до `256` горутин (**FIFO**) однако, есть такая оптимизация, что последняя горутина, может выполнится первой, без очереди, одноэлементный стэк **LIFO**. Это сделано потому, что, есть ненулевая вероятность, того что на **M** контекст выполнения соотвтетсвует тому, что нужен именно этой горутине, поэтому часто имеет смвсл выполнить её вперёд остальных.
  
  
  
  Каждому **P** присваивается **LRQ**, который управляет горутинами, назначенными для выполнения в контексте **P**. Эти горутины по очереди включаются и выключаются из контекста **M**, назначенного для этого **P**. **GRQ** предназначен для горутин, которые не были назначены для **P**. Существует процесс, чтобы переместить горутины из **GRQ** в **LRQ**



![img](../media/go/scheduler_grq_lrq.png)

Каждая локальная очередь проверяет глобальную каждый 61 такт процессора.

```go
runtime.schedule() {
    // only 1/61 of the time, check the global runnable queue for a G.
    // if not found, check the local queue.
    // if not found,
    //     try to steal from other Ps.
    //     if not, check the global runnable queue.
    //     if not found, poll network.
}
```

Это означает, что **P** будет:

- Существует небольшой шанс(**1/61**) немедленно проверить **GRQ** и запустить горутиную оттуда

- приоритетнее запускать **G** в своем собственном **LRQ**

- затем из **LRQ** других **P**. Воруется `1/2` локальной очереди другого(случайного) **P** (по умолчанию, `128` из `256` возможных). Интересная деталь: **work-stealing** в Go не очень агрессивен, поэтому иногда загрузка ядер может быть неравномерной.

- затем из **GRQ**

- затем из **netpoller**. 

Если горутины нет, поток засыпает (`park()`), а `System Monitor` позже его разбудит.

## Work stealing

 В многопоточных вычислениях, возникли две парадигмы в планировании: 

- **Work-sharing**: Когда процессор генерирует новые потоки, он пытается мигрировать их на другие процессоры, в надежде, что они попадут к простаивающему или недостаточно нагруженному процессору.
- **Work-stealing**: Недостаточно нагруженный процессор активно ищет потоки других процессоров и "крадет" некоторые из них.

Когда новая **G** создается или существующая **G** становится готовой к исполнению, она помещается в локальную очередь готовых к исполнению горутин текущего **P**. Когда **P** заканчивается исполнение **G**, он пытается вытащить **G** из своей очереди. Если список пуст, **P** выбирает случайным образом другой процессор (**P**) и пытается украсть половину горутин из его очереди.

## Вытесняющая многозадачность

Для начала напомню, что такое кооперативная и не кооперативная многозадачность.

С не кооперативной (**вытесняющей**) многозадачностью мы все с вами прекрасно знакомы на примере планировщика ОС. Данный планировщик работает в фоне, выгружает потоки на основании различных эвристик, а вместо выгруженных процессорное время начинают получать другие потоки.

Для **кооперативного** планировщика характерно другое поведение — он спит пока одна из горутин явно не разбудит его с намеком о готовности отдать свое место другой. Планировщик далее сам решит, надо ли убирать из контекста текущую горутину, и если да, кого поставить на ее место. Примерно так и работал планировщик GO.

в **GO** 1.14 изменился принцип работы планировщика, рассмотрим причины по которым эти изменения были сделаны. Взгляните на код:

```go
func main() {
    runtime.GOMAXPROCS(1)
    go func() {
        var u int
        for {
            u -= 2
            if u == 1 {
                break
            }
        }
    }()
    <-time.After(time.Millisecond * 5) // в этом месте main горутина разбудит планировщик, а он в свою очередь запустит горутину с циклом

    fmt.Println("go 1.13 has never been here")
}
```

Если скомпилировать его с версией **GO < 1.14**, то строчку «go 1.13 has never been here» вы на экране не увидите. Происходит это потому, что, как только планировщик дает процессорное время горутине с бесконечным циклом, она всецело захватывает P, внутри этой горутины не происходит ни каких вызовов функций, а значит и планировщик мы больше не разбудим. И только явный вызов runtime.Gosched() даст нашей программе завершиться.

В версии до 1.12 runtime Gosched использовал **safe-points** места, где точно можно вызвать планировщик без боязни, что мы попадем в атомарную для GC секцию кода. Как мы уже говорили, данные safe-points располагаются в прологе функции (но далеко не каждой функции, заметьте). Если вы разбирали go-шный ассемблер, то могли бы возразить — никаких очевидных вызовов планировщика там не видно. Да это так, но вы можете найти там инструкцию вызова **runtime.morestack**, а если заглянуть внутрь этой функции то обнаружится вызов планировщика. 

### Блокирующие и неблокирующие системные вызовы

Когда горутина делает `syscall`, **M** блокируется. Чтобы не терять вычислительные ресурсы, **P** временно отвязывается (`handoff`), и System Monitor выделяет новый **M** для работы с этим **P**.

Когда горутина выполняет блокирующий `syscall` (например, файловый ввод/вывод), поток ОС `M` блокируется. Что происходит дальше:

1. `P` *отвязывается* от заблокированного `M` (`handoff()` в `runtime/proc.go`).
2. `P` начинает искать новый `M`. Если свободных `M` нет, создается новый поток ОС.
3. Когда `syscall` завершится, `M` **не возвращается к своему `P`**, а отправляет горутину в глобальную очередь.
4. `M` либо завершает работу, либо берет новую горутину, если есть свободный `P`.

### У `syscall` есть своя очередь?

Нет, у `syscall` нет отдельной очереди. Но есть механизм, похожий на очередь:

1. Когда `syscall` завершается, его `M` **помечается как "ready"**.
2. `M` ставит разблокированную горутину в глобальную очередь или передает ее первому свободному `P`.
3. Если `M` остается без работы, он либо уничтожается (`exit0()`), либо переходит в режим ожидания (`park()`).

**NetPoller** (на базе `epoll/kqueue`) ждет завершения сетевых операций. Если есть активные соединения, он пробуждает соответствующие горутины.

Когда операционная система, на которой вы работаете, имеет возможность обрабатывать системный вызов асинхронно, то, что называется **network poller**, может использоваться для более эффективной обработки системного вызова. Это достигается с помощью **epoll** (Linux), kqueue (MacOS), или iocp (Windows) в этих соответствующих ОС.

Сетевые системные вызовы могут обрабатываться асинхронно многими операционными системами, которые мы используем сегодня. Именно здесь **network poller** показывает себя, поскольку его основное назначение — обработка сетевых операций. Используя **network poller** для сетевых системных вызовов, планировщик может запретить горутинам блокировать **M** при выполнении этих системных вызовов. Это помогает держать **M** доступным для выполнения других горутин в LRQ **P** без необходимости создавать новые **M**. Это помогает уменьшить нагрузку планирования в ОС.



### **Состояния горутин (`G`):**

Горутина (`G`) в Go может находиться в одном из следующих состояний:

| **Состояние** | **Описание**                                                               |
| ------------- | -------------------------------------------------------------------------- |
| `_Gidle`      | Горутина не используется (обычно после завершения).                        |
| `_Grunnable`  | Горутина готова к выполнению, но ждёт в очереди `P`.                       |
| `_Grunning`   | Горутина выполняется на `M`.                                               |
| `_Gsyscall`   | Горутина заблокирована в `syscall` (ожидает завершения системного вызова). |
| `_Gwaiting`   | Горутина ожидает события (например, `channel`, `mutex`, `timer`).          |
| `_Gdead`      | Горутина завершилась и ждёт сборщика мусора.                               |
| `_Gcopystack` | Горутина увеличивает или уменьшает стек.                                   |
| `_Gscan`      | Горутина в процессе GC-сканирования (служебное состояние).                 |

**Как это работает в динамике?**

1. Горутина создаётся (`newproc()`) и становится **`_Grunnable`**.
2. `P` ставит её в локальную очередь.
3. `M` забирает её и запускает → **`_Grunning`**.
4. Если `G` блокируется (`syscall`, ожидание `chan` и т. д.) → **`_Gsyscall` или `_Gwaiting`**.
5. Когда `syscall` или ожидание завершается → горутина возвращается в **`_Grunnable`**.
6. Если горутина завершилась → **`_Gdead`**, и память под неё чистит GC.





*Дополнительно:*

- https://habr.com/ru/users/not91/publications/articles/

- https://habr.com/ru/articles/333654/

- https://habr.com/ru/articles/502506/